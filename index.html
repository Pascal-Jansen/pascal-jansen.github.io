<html lang="en-us" data-lt-installed="true">
    <head>
        <title>Pascal Jansen</title>
        <meta property="og:title" content="Pascal Jansen – Portfolio & Research" />
        <meta property="og:type" content="website" />
        <meta property="og:image" content="https://pascal-jansen.github.io/assets/img/og-image.jpg" />
        <meta property="og:description" content="PhD Candidate and Research Associate at Ulm University specializing in Human-Computer Interaction, Computational Modeling, and Inclusive Design for Ubiquitous Personalization. Visiting Researcher at University College London." />
        <meta property="og:url" content="https://pascal-jansen.github.io/" />
        <meta name="description" content="Discover the work of Pascal Jansen, a PhD Candidate and Research Associate at Ulm University and Visiting Researcher at University College London, focused on HCI, Computational Modeling, and Inclusive Design for Ubiquitous Personalization." />
        <meta name="keywords" content="Pascal Jansen, Ulm University, University College London, HCI, Human-Computer Interaction, Computational Modeling, Inclusive Design, Ubiquitous Personalization, Research, Portfolio" />
        <meta charset="utf-8" />
        <meta name="author" content="Pascal Jansen" />
        <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0" />

        <link rel="stylesheet" type="text/css" href="css/bootstrap.css" />
        <link rel="stylesheet" type="text/css" href="css/animate.css" />
        <link rel="stylesheet" type="text/css" href="css/main.css" />

        <script type="text/javascript" src="js/jquery.js"></script>
        <script type="text/javascript" src="js/scrollTo.js"></script>
        <script type="text/javascript" src="js/wow.js"></script>
        <script type="text/javascript" src="js/parallax.js"></script>
        <script type="text/javascript" src="js/headhesive.js"></script>
        <script type="text/javascript" src="js/main.js"></script>
    </head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N465H94Q6K"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-N465H94Q6K');
</script>
    
    <body data-new-gr-c-s-check-loaded="14.1215.0" data-gr-ext-installed="">
        <header class="banner">
            <div class="container">
                <div class="logo pull-left">
                    Pascal Jansen
                </div>

                <nav class="pull-right">
                    <ul class="list-unstyled">
                        <li><a target="_blank" href="data/CV_Jansen_2025.pdf">CV</a></li>
                        <li><a target="_blank" href="mailto:pascal.jansen@uni-ulm.de">E-Mail</a></li>
                    </ul>
                </nav>
            </div>
        </header>

        <!--  Bio Section  -->
        <section class="bio" id="bio" style="background-color: rgb(224 224 224); text-align: left;">
            <div class="container">
                <div class="row equal-height">
                    <!-- Left column with text -->
                    <div class="col-md-8 panel">
                        <div class="caption">
                            <br />
                            <h1 class=" " style="margin-top: 0;">Pascal Jansen</h1>
                        </div>

                        <p class=" ">
                            <span>pascal.jansen (at) uni-ulm.de</span>
                        </p>
                        <br />
                        <p class=" ">
                            <span>PhD Candidate and Research Associate at <a target="_blank" href="https://www.uni-ulm.de/en/in/mi/institute/staff/pascal-jansen">Ulm University</a>, <br>Institute of Media Informatics, Human-Computer Interaction Group</span><br><br>
                            <span>previously Visiting Researcher at <a target="_blank" href="https://www.ucl.ac.uk/uclic/">University College London, Interaction Centre</a></span>
                        </p>
                        <br />
                        <div class="agenda  ">
                            <div class="bio-text-title"><h2>Ubiquitous Personalization</h2></div>
                            <p>
                                My research combines <strong>Human-Computer Interaction (HCI)</strong>, 
                                <strong>Computational Modeling</strong>, and <strong>Inclusive Design</strong> to create technology in everyday environments that is seamlessly personalized to each individual's states, preferences, 
                                and needs.<br>A key challenge lies in developing systems that effectively respond 
                                to diverse user abilities and hardly predictable use contexts. By advancing 
                                <strong>computational user interface optimization</strong>, 
                                <strong>interaction simulation and analysis</strong>, and 
                                <strong>inclusive design</strong>, I make 
                                technology more intuitive, adaptive, and accessible for all.
                              </p>
                            <br />
                            <!-- Left-aligned links -->
                            <div class="horizontal-list bio-link-list-wide">
                                <a target="_blank" href="data/CV_Jansen_2025.pdf">Curriculum vitae (April 2025)</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://scholar.google.de/citations?user=cR1_0-EAAAAJ&hl=en">Scholar Profile</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://www.linkedin.com/in/pascal-jansen-/">LinkedIn</a>
                                <span class="separator">/</span>
                                <!--<a target="_blank" href="https://x.com/pascal_jansen_">X</a>
                                <span class="separator">/</span>-->
                                <a target="_blank" href="https://github.com/Pascal-Jansen">GitHub</a>
                            </div>
                            <div class="horizontal-list bio-link-list-narrow">
                                <a target="_blank" href="data/CV_Jansen_2025.pdf">CV</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://scholar.google.de/citations?user=cR1_0-EAAAAJ&hl=en">Scholar</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://www.linkedin.com/in/pascal-jansen-/">LinkedIn</a>
                                <span class="separator">/</span>
                                <!--<a target="_blank" href="https://x.com/pascal_jansen_">X</a>
                                <span class="separator">/</span>-->
                                <a target="_blank" href="https://github.com/Pascal-Jansen">GitHub</a>
                            </div>
                        </div>
                    </div>

                    <div class="col-md-4 align-bottom">
                        <img src="images/uu-jansen-transparent.png" alt="Profile Picture" class="profile-image" />
                    </div>
                </div>
            </div>
        </section>
        <!--  End Bio Section  -->

        <a target="_blank" id="showHere"></a>

        <!--  Publication Section  -->
        <section class="publication" id="publication">
            <div class="container">
     
                <h1>News</h1>
                <!-- Top 10 entries -->
                <div class="row" style="margin-bottom: 0;">
                <div class="col-md-3"><h4 style="margin-top: 0;">2025 May</h4></div>
                <div class="col-md-9">
                    Transportation Research Part F: Traffic Psychology and Behaviour paper accepted: <em>Longitudinal Effects of Visualizing Uncertainty of Situation Detection and Prediction of Automated Vehicles (AVs) on User Perceptions</em>.
                    A three-day study with 50 participants who viewed real-world commute videos twice daily showed that clear, consistent AV uncertainty visualizations significantly increased trust over time while revealing the need for explicit AV intent cues and manual intervention options.
                </div>
                </div>

                <div class="row" style="margin-bottom: 0;">
                <div class="col-md-3"><h4 style="margin-top: 0;">2025 April</h4></div>
                <div class="col-md-9">
                    Our paper <a href="https://dl.acm.org/doi/full/10.1145/3706598.3713514" target="_blank">OptiCarVis</a> received an Honorable Mention Award at CHI '25 in Yokohama, Japan. <br>
                    At CHI '25, <strong>five papers</strong> I led or co-authored—spanning sustainability, the impact of motion on interaction quality, and adaptive user interfaces for future mobility—were published.<br>
                    I also delivered the laudatio for Mark Colley at the <strong>SIGCHI Awards Dinner</strong>, honoring his Special Recognition for Early Career Researcher Award.
                </div>
                </div>

                <div class="row" style="margin-bottom: 0;">
                <div class="col-md-3"><h4 style="margin-top: 0;">2025 March</h4></div>
                <div class="col-md-9">
                    Two papers published at HRI '25 in Melbourne, Australia:  
                    <a href="https://ieeexplore.ieee.org/abstract/document/10974171" target="_blank">HUD-SUMO</a> links SUMO and CARLA to simulate AR windshield HUD settings and predict their impact on reaction time, speed adherence, lane changes, and acceleration in dense traffic.
                    <a href="https://ieeexplore.ieee.org/abstract/document/10973993" target="_blank">UAM-SUMO</a> extends SUMO to model urban air taxi corridors alongside ground vehicles, enabling large-scale studies of traffic flow, mode choice, and passenger trust.
                </div>
                </div>

                <div class="row" style="margin-bottom: 0;">
                <div class="col-md-3"><h4 style="margin-top: 0;">2025 February</h4></div>
                <div class="col-md-9">
                    At the <a href="https://www.instagram.com/p/DGVPcRkIMKA/?img_index=3" target="_blank">Bildungsmesse (education fair) Ulm 2025</a>, our <em>VeMoR</em> simulator drew many prospective students eager to experience high-immersion motion feedback in a safe, lab-grade environment.  
                    We were honored when the speaker of the Baden-Württemberg parliament and Ulm's Mayor visited our booth to see and discuss the future of education and research.
                </div>
                </div>

                <div class="row" style="margin-bottom: 0;">
                <div class="col-md-3"><h4 style="margin-top: 0;">2025 January</h4></div>
                <div class="col-md-9">
                    I started as a visiting researcher at <a href="https://www.ucl.ac.uk/uclic/" target="_blank">UCL Interaction Centre</a> in London, UK hosted by <a href="https://scholar.google.de/citations?user=Kt5I7wYAAAAJ&hl=en&oi=ao" target="_blank">Mark Colley</a>. 
                </div>
                </div>

                <div class="row" style="margin-bottom: 0;">
                <div class="col-md-3"><h4 style="margin-top: 0;">2024 June</h4></div>
                <div class="col-md-9">
                    At the <a href="https://www.uni-ulm.de/universitaet/hochschulkommunikation/veranstaltungen/langer-abend-der-wissenschaft/" target="_blank">“Long Evening of Science” Fair</a> at Ulm University, I showcased <em>VeMoR</em>, our VR vehicle-motion simulator enabling roll, pitch, and yaw synchronization to the virtual vehicle to bridge the gap between static lab setups and expensive full-motion rigs.  
                    I introduced our lab's future mobility research to over 2 000 visitors—many prospective students and families—sparking enthusiasm and discussion about science and studying at the university.
                </div>
                </div>

                <div class="row" style="margin-bottom: 0;">
                <div class="col-md-3"><h4 style="margin-top: 0;">2024 May</h4></div>
                <div class="col-md-9">
                    Transportation Research Part F: Traffic Psychology and Behaviour paper accepted: <a href="https://www.sciencedirect.com/science/article/pii/S1369847824001141" target="_blank">Visualizing Imperfect Situation Detection and Prediction in Automated Vehicles: Understanding Users' Perceptions via User-Chosen Scenarios</a>.  
                    In this work, we created EduLicit, a web-based platform that both elicits the public's most challenging AV scenarios and evaluates how visualizing an AV's perception, prediction, and planning processes influence user acceptance, validated via two online studies.
                </div>
                </div>

                <div class="row" style="margin-bottom: 0;">
                <div class="col-md-3"><h4 style="margin-top: 0;">2024 January</h4></div>
                <div class="col-md-9">
                    Our award-winning VR game <a href="https://store.steampowered.com/app/2746510/The_Social_Engineer/" target="_blank">The Social Engineer</a> launched on Steam. 
                    The Social Engineer is a room-scale VR serious game that immerses players in realistic social-engineering scenarios. It enables exploring companies, exploiting vulnerabilities, and practicing defensive strategies through interactive narrative missions. 
                </div>
                </div>

                <!-- Hidden extra entries -->
                <div id="moreNews" style="display: none;">
                <div class="row" style="margin-bottom: 0;">
                <div class="col-md-3"><h4 style="margin-top: 0;"></h4></div>
                <div class="col-md-9">
                    <a href="https://dl.acm.org/doi/abs/10.1145/3610977.3637478" target="_blank">PedSUMO: Simulacra of Automated Vehicle-Pedestrian Interaction Using SUMO To Study Large-Scale Effects</a> accepted at HRI ’24.  
                    PedSUMO is a SUMO extension that simulates pedestrian interactions at unsignalized crossings to study how external automated vehicle signals affect large-scale pedestrian compliance, validated with data from Ingolstadt, Germany. The source-code is available on <a href="https://github.com/M-Colley/pedsumo" target="_blank">GitHub</a>.
                </div>
                </div>

                <div class="row" style="margin-bottom: 0;">
                <div class="col-md-3"><h4 style="margin-top: 0;">2023 September</h4></div>
                <div class="col-md-9">
                    <a href="https://www.auto-ui.org/23/organizers/" target="_blank">Registration Chair</a> at AutoUI ’23 in Ingolstadt, overseeing the online and on-site attendee registration process. I communicated with ACM regarding the conference organization and helped finalize the conference budget.
                </div>
                </div>

                <div class="row" style="margin-bottom: 0;">
                    <div class="col-md-3"><h4 style="margin-top: 0;">2023 April</h4></div>
                    <div class="col-md-9">
                    First-author CHI '23 paper published: <a href="https://dl.acm.org/doi/full/10.1145/3544548.3580760" target="_blank">AutoVis: Enabling Mixed-Immersive Analysis of Automotive UI Interaction Studies</a>. 
                    AutoVis is an analytics environment—combining desktop and VR views with automotive-specific visualizations like context portals, driving-path events, avatars, trajectories, and heatmaps.
                    This was guided by expert-derived requirements and verified through prototype walkthroughs, using a real vehicle, and on a public dataset. See our <a href="https://autovis-demo.onrender.com/" target="_blank">demo website</a> for more information.
                    </div>
                </div>
                </div>

                <button id="moreBtn" class="btn btn-primary">More News</button>

                <script>
                document.getElementById('moreBtn').addEventListener('click', function() {
                    var extra = document.getElementById('moreNews');
                    if (extra.style.display === 'none') {
                    extra.style.display = 'block';
                    this.textContent = 'Show Less';
                    } else {
                    extra.style.display = 'none';
                    this.textContent = 'More News';
                    }
                });
                </script>

                <hr>
                <h1>Research Agenda</h1>
                <div class="row">
                    <div class="col-md-12">
                        <div class="venn-wrapper">
                        <div class="css-art--3-pie-venn-diagram">
                            <div class="css-art--pie css-art--pie-1">
                                <h3>Ubiquitous<br>User Interfaces</h3>
                                <ul>
                                <li><a href="https://dl.acm.org/doi/abs/10.1145/3379337.3415843" target="_blank">Mixed-Reality</a></li>
                                <li><a href="https://dl.acm.org/doi/abs/10.1145/3534617" target="_blank">Automated Vehicles</a></li>
                                <li><a href="https://dl.acm.org/doi/full/10.1145/3706598.3713288" target="_blank">Urban Air Mobility</a></li>
                                <li><a href="https://dl.acm.org/doi/10.1145/3706598.3713180" target="_blank">Human-Robot Interaction</a></li>
                                </ul>
                            </div>
                            <div class="pie-1-2-intersect">
                                Accessible UIs<br>for everyone,<br>everywhere
                            </div>
                            <div class="css-art--pie css-art--pie-2">
                                <h3>Inclusive<br>Design</h3>
                                <ul>
                                <li><a href="https://dl.acm.org/doi/abs/10.1145/3492802" target="_blank">User State Assessment</a></li>
                                <li><a href="https://dl.acm.org/doi/10.1145/3643558" target="_blank">Sustainability</a></li>
                                <li><a href="https://www.sciencedirect.com/science/article/pii/S1369847824001141" target="_blank">Community Engagement</a></li>
                                <li><a href="https://dl.acm.org/doi/abs/10.1145/3383668.3419917" target="_blank">Serious Games & Education</a></li>
                                </ul>
                            </div>
                            <div class="pie-2-3-intersect">
                                Models and simulations of the individual user
                            </div>
                            <div class="css-art--pie css-art--pie-3">
                                <h3>Computational<br>Modeling & Simulation</h3>    
                                <ul>
                                <li><a href="https://dl.acm.org/doi/10.1145/3706598.3713514" target="_blank">Human-in-the-Loop Bayesian Optimization</a></li>
                                <li><a href="https://ieeexplore.ieee.org/abstract/document/10974171" target="_blank">Simulation of Users</a></li>
                                <li><a href="https://dl.acm.org/doi/abs/10.1145/3494968" target="_blank">Novel Simulators</a></li>
                                </ul>
                            </div>
                            <div class="pie-3-1-intersect">
                                Context-robust computational UI<br>design and interaction
                            </div>
                            <div class="pie-main-intersection">
                                Pascal Jansen
                            </div>
                        </div>
                        </div>

                        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700,700i" rel="stylesheet">
                    
                        <p>
                            <strong>Ubiquitous computing</strong> is reshaping peoples' interaction with the world—from mixed-reality workspaces to autonomous cars, and service robots.
                            Yet many of these systems fit most but are not optimal for every individual when devices, tasks, environments, or user states shift.
                            My multidisciplinary research bridges <strong>Human-Computer Interaction</strong>, <strong>Inclusive Design</strong>, and <strong>Computational Modeling</strong> to achieve three objectives:
                        </p>
                                <strong>Accessible UIs for everyone, everywhere</strong>
                                <p>
                                    Addressing the exclusion of users with sensory, cognitive, or situational constraints by designing and evaluating software and hardware interfaces with appropriate interaction modalities.
                                </p>

                                <strong>Models and simulations of the individual user</strong>
                                <p>
                                    Overcoming barriers of “average-user” design assumptions through data-driven models that capture demographic, cognitive, and motor diversity, enabling human-in-the-loop design optimization instead of resource-intensive traditional development cycles.
                                </p>

                                <strong>Context-robust computational UI design and interaction</strong>
                                <p>
                                    Based on the developed systems and studies, creating adaptive UIs that include individual users and are accessible in every context.
                                </p>
                    </div>
                </div>


                <hr>
                <h1>Publications (Excerpt)</h1>

                     <div class="row">
                        <div class="col-md-4 text-center  " style="visibility: visible;  ">
                            <img src="images/long-vis.png" />
                        </div>
                        
                        <div class="col-md-8  " style="visibility: visible;  ">
                            <h2>Longitudinal Effects of Visualizing Uncertainty of Situation Detection and Prediction of Automated Vehicles on User Perceptions</h2>
                            
                            <p> 
                                <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen*</a></u>, Mark Colley*, Max Rädler*, Jonas Schwedler, and Enrico Rukzio (<i>*joint first-author</i>)
                            </p>
                            
                            <p><strong>TRF '25</strong>: Transportation Research Part F: Psychology and Behavior</p>
                        
                            <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                    <!-- Abstract Button -->
                                    <button onclick="toggleAbstract(this, 'abstract-20')" class="abstract-btn">Abstract</button> /
                                    <a target="_blank" href="data/publications/longitudinal-visualization.pdf" class="paper">Paper</a> /
                                    <a target="_blank" href="data/videos/long-vis-video.mp4" class="video">Video</a> /
                                    <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S1369847825001779">Elsevier</a>
                            </div>
                            
                            <!-- Hidden Abstract -->
                            <p id="abstract-20" style="display: none; padding-top: 5px; text-align: justify;">
                                This paper explores the impact of uncertainty visualizations in automated vehicle (AV) functionality on user perceptions over a three-day longitudinal study. Participants (N=50) watched real-world driving videos twice daily, in the morning and evening. These videos depicted morning and evening commutes, featuring visualizations of AVs' pedestrian detection, vehicle recognition, and pedestrian intention prediction. We measured perceived safety, trust, mental workload, and cognitive load using a within-subjects design. Results show increased perceived safety and trust over time, with higher ratings in the evening sessions, reflecting greater predictability and user confidence in AV by the study's end. However, inconsistencies in pedestrian detection and intention prediction led to mixed reactions, highlighting the need for visualization stability and clarity refinement. Participants also desired a feature indicating the AV's intended path and options for manual intervention. Our findings suggest transparency and usability in AV visualizations can foster trust and perceived safety, informing future AV interface design.
                            </p>
                        </div>
                    </div>

                
                    <div class="row">
                        <div class="col-md-4 text-center  " style="visibility: visible;  ">
                            <img src="images/hud-sumo.png" />
                        </div>
                        
                        <div class="col-md-8  " style="visibility: visible;  ">
                            <h2>HUD-SUMO: Simulacra of In-Vehicle Head-Up Displays Using SUMO To Study Large-Scale Effects</h2>
                            
                            <p> 
                                <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen*</a></u>, Mark Colley*, Elisabeth Wimmer, Jan Maresch, and Enrico Rukzio (<i>*joint first-author</i>)
                            </p>
                            
                            <p><strong>HRI '25</strong>: Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</p>
                        
                            <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                    <!-- Abstract Button -->
                                    <button onclick="toggleAbstract(this, 'abstract-19')" class="abstract-btn">Abstract</button> /
                                    <a target="_blank" href="data/publications/hud-sumo-hri25.pdf" class="paper">Paper</a> /
                                    <a target="_blank" href="data/videos/hud-sumo-video.mp4" class="video">Video</a> /
                                    <a target="_blank" href="https://dl.acm.org/doi/abs/10.5555/3721488.3721614" class="acm">ACM</a>
                            </div>
                            
                            <!-- Hidden Abstract -->
                            <p id="abstract-19" style="display: none; padding-top: 5px; text-align: justify;">
                                Large-scale effects of head-up displays (HUDs) are currently unknown, as experiments focus primarily on empirical data from one participant. Therefore, we simulate the impact of augmented reality (AR) windshield HUDs on driving performance. Using the open-source simulators SUMO and CARLA, we model various AR HUD settings, such as fatigue, and assess their effects on key driving factors such as reaction time, speed adherence, lane-changing behavior, and acceleration. The HUD settings include brightness, information frequency, field of view, and relevance of displayed information. The simulation provides insight into how different HUD configurations may influence driving behavior, contributing to future vehicle design and safety guidelines for AR HUDs.
                            </p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-md-4 text-center  " style="visibility: visible;  ">
                            <img src="images/opticarvis.png" />
                        </div>
                        
                        <div class="col-md-8  " style="visibility: visible;  ">
                            <h2>OptiCarVis: Improving Automated Vehicle Functionality Visualizations Using Bayesian Optimization to Enhance User Experience</h2>
                            
                            <p> 
                                <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen*</a></u>, Mark Colley*, Svenja Krauß, Daniel Hirschle, and Enrico Rukzio (<i>*joint first-author</i>)
                            </p>

                            <!-- Conference Name with Custom Badge Behind -->
                            <div style="position: relative; display: inline-block;">
                                <p style="position: relative; z-index: 2; margin: 0;">
                                   <strong>CHI '25</strong>: Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems<b><div class="award">Honorable Mention Award</div></b>
                                </p>
                            </div>
                        
                            <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                    <!-- Abstract Button -->
                                    <button onclick="toggleAbstract(this, 'abstract-18')" class="abstract-btn">Abstract</button> /
                                    <a target="_blank" href="data/publications/CHI25_OptiCarVis.pdf" class="paper">Paper</a> /
                                    <a target="_blank" href="data/videos/opticar-video-figure.mp4" class="video">Video</a> /
                                    <a target="_blank" href="https://dl.acm.org/doi/full/10.1145/3706598.3713514" class="acm">ACM</a>
                            </div>
                            
                            <!-- Hidden Abstract -->
                            <p id="abstract-18" style="display: none; padding-top: 5px; text-align: justify;">
                                Automated vehicle (AV) acceptance relies on their understanding via feedback. While visualizations aim to enhance user understanding of AV's detection, prediction, and planning functionalities, establishing an optimal design is challenging. Traditional "one-size-fits-all" designs might be unsuitable, stemming from resource-intensive empirical evaluations. This paper introduces OptiCarVis, a set of Human-in-the-Loop (HITL) approaches using Multi-Objective Bayesian Optimization (MOBO) to optimize AV feedback visualizations. We compare conditions using eight expert and user-customized designs for a Warm-Start HITL MOBO. An online study (N=117) demonstrates OptiCarVis's efficacy in significantly improving trust, acceptance, perceived safety, and predictability without increasing cognitive load. OptiCarVis facilitates a comprehensive design space exploration, enhancing in-vehicle interfaces for optimal passenger experiences and broader applicability.
                            </p>
                        </div>
                    </div>


                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/eHMI-BO.png" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>Improving External Communication of Automated Vehicles Using Bayesian Optimization</h2>
                        
                        <p> 
                            Mark Colley*, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen*</a></u>, Mugdha Keskar, and Enrico Rukzio (<i>*joint first-author</i>)
                        </p>
                        
                        <p><strong>CHI '25</strong>: Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-17')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/CHI25_eHMI_BO.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="data/videos/ehmi-video-figure.mp4" class="video">Video</a> /
                                <a target="_blank" href="https://dl.acm.org/doi/full/10.1145/3706598.3714187" class="acm">ACM</a>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-17" style="display: none; padding-top: 5px; text-align: justify;">
                            The absence of a human operator in automated vehicles (AVs) may require external Human-Machine Interfaces (eHMIs) to facilitate communication with other road users in uncertain scenarios, for example, regarding the right of way.
                            Given the plethora of adjustable parameters, balancing visual and auditory elements is crucial for effective communication with other road users. With N=37 participants, this study employed multi-objective Bayesian optimization to enhance eHMI designs and improve trust, safety perception, and mental demand. By reporting the Pareto front, we identify optimal design trade-offs. This research contributes to the ongoing standardization efforts of eHMIs, supporting broader adoption.
                        </p>
                    </div>
                </div>


                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/uam-BO.png" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>Fly Away: Evaluating the Impact of Motion Fidelity on Optimized User Interface Design via Bayesian Optimization in Automated Urban Air Mobility Simulations</h2>
                        
                        <p> 
                            Luca-Maxim Meinhardt, Clara Schramm, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Mark Colley, and Enrico Rukzio
                        </p>
                        
                        <p><strong>CHI '25</strong>: Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-16')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/CHI2025___UAM_BO.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="data/videos/uam-bo-video-figure.mp4" class="video">Video</a> /
                                <a target="_blank" href="https://dl.acm.org/doi/full/10.1145/3706598.3713288" class="acm">ACM</a>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-16" style="display: none; padding-top: 5px; text-align: justify;">
                            Automated Urban Air Mobility (UAM) can improve passenger transportation and reduce congestion, but its success depends on passenger trust. While initial research addresses passengers' information needs, questions remain about how to simulate air taxi flights and how these simulations impact users and interface requirements.
                            We conducted a between-subjects study (N=40), examining the influence of motion fidelity in Virtual-Reality-simulated air taxi flights on user effects and interface design. Our study compared simulations with and without motion cues using a 3-Degrees-of-Freedom motion chair. Optimizing the interface design across six objectives, such as trust and mental demand, we used multi-objective Bayesian optimization to determine the most effective design trade-offs.
                            Our results indicate that motion fidelity decreases users' trust, understanding, and acceptance, highlighting the need to consider motion fidelity in future UAM studies to approach realism. However, minimal evidence was found for differences or equality in the optimized interface designs, suggesting personalized interface designs.
                        </p>
                    </div>
                </div>


                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/bumpy-ride.png" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>Bumpy Ride? Understanding the Effects of External Forces on Spatial Interactions in Moving Vehicles</h2>
                        
                        <p> 
                            Markus Sasalovici, Albin Zeqiri, Robin Connor Schramm, Oscar Javier Ariza Nuñez, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Jann Philipp Freiwald, Mark Colley, Christian Winkler, and Enrico Rukzio
                        </p>
                        
                        <p><strong>CHI '25</strong>: Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-15')" class="abstract-btn">Abstract</button>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-15" style="display: none; padding-top: 5px; text-align: justify;">
                            As the use of Head-Mounted Displays in moving vehicles increases, passengers can immerse themselves in visual experiences independent of their physical environment. However, interaction methods are susceptible to physical motion, leading to input errors and reduced task performance. This work investigates the impact of Gforces, vibrations, and unpredictable maneuvers on 3D interaction
                            methods. We conducted a field study with 24 participants in both stationary and moving vehicles to examine the effects of vehicle motion on four interaction methods: (1) Gaze&Pinch, (2) DirectTouch,
                            (3) Handray, and (4) HeadGaze. Participants performed selections in a Fitts’ Law task. Our findings reveal a significant effect of vehicle motion on interaction accuracy and duration across the tested combinations of Interaction Method × Road Type × Curve Type. We
                            found a significant impact of movement on throughput, error rate, and perceived workload. Finally, we propose future research considerations and recommendations on interaction methods during vehicle movement.    
                        </p>
                    </div>
                </div>


                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/plantpal.jpg" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>PlantPal: Leveraging Precision Agriculture Robots to Facilitate Remote Engagement in Urban Gardening</h2>
                        
                        <p> 
                            Albin Zeqiri, Julian Britten, Clara Schramm, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Michael Rietzler, and Enrico Rukzio
                        </p>
                        
                        <p><strong>CHI '25</strong>: Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-14')" class="abstract-btn">Abstract</button>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-14" style="display: none; padding-top: 5px; text-align: justify;">
                            Urban gardening is widely recognized for its numerous health and environmental benefits. However, the lack of suitable garden spaces, demanding daily schedules, and limited gardening expertise present major roadblocks for citizens looking to engage in urban gardening. While prior research has explored smart home solutions to support urban gardeners, these approaches currently do not fully address these practical barriers. In this paper, we present PlantPal, a system that enables the cultivation of garden spaces irrespective of one's location, expertise level, or time constraints. PlantPal enables the shared operation of a precision agriculture robot (PAR) that is equipped with garden tools and a multi-camera system. Insights from a 3-week deployment (N=18) indicate that PlantPal facilitated the integration of gardening tasks into daily routines, fostered a sense of connection with one's field, and provided an engaging experience despite the remote setting. We contribute design considerations for future robot-assisted urban gardening concepts.
                        </p>
                    </div>
                </div>


                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/edulicit.png" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>Visualizing Imperfect Situation Detection and Prediction in Automated Vehicles: Understanding Users’ Perceptions via User-Chosen Scenarios</h2>
                        
                        <p> 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen*</a></u>, Mark Colley*, Tim Pfeifer, and Enrico Rukzio (<i>*joint first-author</i>)
                        </p>
                        
                        <p><strong>TRF '24</strong>: Transportation Research Part F: Psychology and Behavior</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-13')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/edulicit.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S1369847824001141">Elsevier</a>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-13" style="display: none; padding-top: 5px; text-align: justify;">
                            User acceptance is essential for successfully introducing automated vehicles (AVs). Understanding the technology is necessary to overcome skepticism and achieve acceptance. This could be achieved by visualizing (uncertainties of) AV's internal processes, including situation perception, prediction, and trajectory planning. At the same time, relevant scenarios for communicating the functionalities are unclear. Therefore, we developed EduLicitto concurrently elicit relevant scenarios and evaluate the effects of visualizing AV's internal processes. A website capable of showing annotated videos enabled this methodology. With it, we replicated the results of a previous online study (N=76) using pre-recorded real-world videos. Additionally, in a second online study (N=22), participants uploaded scenarios they deemed challenging for AVs using our website. Most scenarios included large intersections and/or multiple vulnerable road users. Our work helps assess scenarios perceived as challenging for AVs by the public and, simultaneously, can help educate the public about visualizations of the functionalities of current AVs.
                        </p>
                    </div>
                </div>



                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/ped-sumo.png" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>PedSUMO: Simulacra of Automated Vehicle-Pedestrian Interaction Using SUMO To Study Large-Scale Effects</h2>
                        
                        <p> 
                            Mark Colley, Julian Czymmeck, Mustafa Kücükkocak, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, and Enrico Rukzio
                        </p>
                        
                        <p><strong>HRI '24</strong>: Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-12')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/ped-sumo.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3610977.3637478" class="acm">ACM</a>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-12" style="display: none; padding-top: 5px; text-align: justify;">
                            As automated vehicles become more widespread but lack a driver to communicate in uncertain situations, external communication, for example, via LEDs or displays, is evaluated. However, the concepts are mostly evaluated in simple scenarios, such as one person trying to cross in front of one automated vehicle. The traditional empirical approach fails to study the large-scale effects of these in this not-yet-real scenario. Therefore, we built PedSUMO, an enhancement to SUMO for the simulacra of automated vehicles' effects on public traffic, specifically how pedestrian attributes affect their respect for automated vehicle priority at unprioritized crossings. We explain the algorithms used and the derived parameters relevant to the crossing. We open-source our code under https://github.com/M-Colley/pedsumo and demonstrate an initial data collection and analysis of Ingolstadt, Germany.
                        </p>
                    </div>
                </div>



                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/esfs.png" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>'Eco Is Just Marketing': Unraveling Everyday Barriers to the Adoption of Energy-Saving Features in Major Home Appliances</h2>
                        
                        <p> 
                            Albin Zeqiri, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Jan Ole Rixen, Michael Rietzler, and Enrico Rukzio
                        </p>
                        
                        <p><strong>IMWUT '24</strong>: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-11')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/esfs.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3643558" class="acm">ACM</a>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-11" style="display: none; padding-top: 5px; text-align: justify;">
                            Energy-saving features (ESFs) represent a simple way to reduce the resource consumption of home appliances (HAs), yet they remain under-utilized. While prior research focused on increasing the use of ESFs through behavior change interventions, there is currently no clarity on the barriers that restrict their utilization in the first place. To bridge this gap, we conducted a qualitative analysis of 349 Amazon product reviews and 98 Reddit discussions, yielding three qualitative themes that showcase how users perceive, interact with, and evaluate ESFs in HAs. Based on these themes, we derived frequent barriers to ESF adoption, which guided a subsequent expert focus group (N=5) to assess the suitability of behavior change interventions and potential alternative strategies for ESF adoption. Our findings deepen the understanding of everyday barriers surrounding ESFs and enable the targeted design and assessment of interventions for future HAs.
                        </p>
                    </div>
                </div>

<!--
                <div class="row">
                    <div class="col-md-6 text-center  " style="visibility: visible;  ">
                        <img src="images/eye-gaze-game.png" />
                    </div>
                    
                    <div class="col-md-6  " style="visibility: visible;  ">
                        <h2>Effects of a Gaze-Based 2D Platform Game on User Enjoyment, Perceived Competence, and Digital Eye Strain</h2>
                        
                        <p> 
                            Mark Colley, Beate Wanner, Max Rädler, Marcel Rötzer, Julian Frommel, Teresa Hirzle, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, and Enrico Rukzio
                        </p>
                        
                        <p><strong>CHI '24</strong>: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <button onclick="toggleAbstract(this, 'abstract-10')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/eye-gaze-game.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="data/videos/eye-gaze-game.mp4" class="video">Video</a> /
                                <a target="_blank" href="https://dl.acm.org/doi/full/10.1145/3613904.3641909" class="acm">ACM</a>
                        </div>
                        
                        <p id="abstract-10" style="display: none; padding-top: 5px; text-align: justify;">
                            Gaze interaction is a promising interaction method to increase variety, challenge, and fun in games. We present “Shed Some Fear”, a 2D platform game including numerous eye-gaze-based interactions. “Shed Some Fear” includes control with eye-gaze and traditional keyboard input. The eye-gaze interactions are partially based on eye exercises reducing digital eye strain but also on employing peripheral vision. By employing eye-gaze as a necessary input mechanism, we explore the effects on and tradeoffs between user enjoyment and digital eye strain in a five-day longitudinal between-subject study (N=17) compared to interaction with a traditional mouse. We found that perceived competence was significantly higher with eye gaze interaction and significantly higher internal eye strain. With this work, we contribute to the not straightforward inclusion of eye tracking as a useful and fun input method for games.
                        </p>
                    </div>
                </div>
            -->


                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/AutoVis.png" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>AutoVis: Enabling Mixed-Immersive Analysis of Automotive User Interface Interaction Studies</h2>
                        
                        <p> 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Julian Britten, Alexander Häusele, Thilo Segschneider, Mark Colley, and Enrico Rukzio
                        </p>
                        
                        <p><strong>CHI '23</strong>: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-9')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/AutoVis.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="data/videos/AutoVis.mp4" class="video">Video</a> /
                                <a target="_blank" href="https://dl.acm.org/doi/full/10.1145/3544548.3580760" class="acm">ACM</a>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-9" style="display: none; padding-top: 5px; text-align: justify;">
                            Automotive user interface (AUI) evaluation becomes increasingly complex due to novel interaction modalities, driving automation, heterogeneous data, and dynamic environmental contexts. Immersive analytics may enable efficient explorations of the resulting multilayered interplay between humans, vehicles, and the environment. However, no such tool exists for the automotive domain. With AutoVis, we address this gap by combining a non-immersive desktop with a virtual reality view enabling mixed-immersive analysis of AUIs. We identify design requirements based on an analysis of AUI research and domain expert interviews (N=5). AutoVis supports analyzing passenger behavior, physiology, spatial interaction, and events in a replicated study environment using avatars, trajectories, and heatmaps. We apply context portals and driving-path events as automotive-specific visualizations. To validate AutoVis against real-world analysis tasks, we implemented a prototype, conducted heuristic walkthroughs using authentic data from a case study and public datasets, and leveraged a real vehicle in the analysis process.
                        </p>
                    </div>
                </div>



                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/auto-design-space.png" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review</h2>
                        
                        <p> 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Mark Colley, and Enrico Rukzio
                        </p>
                        
                        <p><strong>IMWUT '22</strong>: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-8')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/auto-design-space.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3534617" class="acm">ACM</a> /
                                <a target="_blank" href="https://in-vehicle-interaction-design-space.onrender.com/" class="paper">Website</a>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-8" style="display: none; padding-top: 5px; text-align: justify;">
                            Automotive user interfaces constantly change due to increasing automation, novel features, additional applications, and user demands. While in-vehicle interaction can utilize numerous promising modalities, no existing overview includes an extensive set of human sensors and actuators and interaction locations throughout the vehicle interior. We conducted a systematic literature review of 327 publications leading to a design space for in-vehicle interaction that outlines existing and lack of work regarding input and output modalities, locations, and multimodal interaction. To investigate user acceptance of possible modalities and locations inferred from existing work and gaps unveiled in our design space, we conducted an online study (N=48). The study revealed users' general acceptance of novel modalities (e.g., brain or thermal activity) and interaction with locations other than the front (e.g., seat or table). Our work helps practitioners evaluate key design decisions, exploit trends, and explore new areas in the domain of in-vehicle interaction.
                        </p>
                    </div>
                </div>


<!--
                <div class="row">
                    <div class="col-md-6 text-center  " style="visibility: visible;  ">
                        <img src="images/eye-strain.png" />
                    </div>
                    
                    <div class="col-md-6  " style="visibility: visible;  ">
                        <h2>Understanding, Addressing, and Analysing Digital Eye Strain in Virtual Reality Head-Mounted Displays</h2>
                        
                        <p> 
                            Teresa Hirzle, Fabian Fischbach, Julian Karlbauer, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Jan Gugenheimer, Enrico Rukzio, and Andreas Bulling
                        </p>
                        
                        <p><strong>TOCHI '22</strong>: ACM Transactions on Computer-Human Interaction</p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                
                                <button onclick="toggleAbstract(this, 'abstract-7')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/eye-strain.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/full/10.1145/3492802" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        
                        <p id="abstract-7" style="display: none; padding-top: 5px;">
                            Digital eye strain (DES), caused by prolonged exposure to digital screens, stresses the visual system and negatively affects users’ well-being and productivity. While DES is well-studied in computer displays, its impact on users of virtual reality (VR) head-mounted displays (HMDs) is largely unexplored—despite that some of their key properties (e.g., the vergence-accommodation conflict) make VR-HMDs particularly prone. This work provides the first comprehensive investigation into DES in VR HMDs. We present results from a survey with 68 experienced users to understand DES symptoms in VR-HMDs. To help address DES, we investigate eye exercises resulting from survey answers and blue light filtering in three user studies (N = 71). Results demonstrate that eye exercises, but not blue light filtering, can effectively reduce DES. We conclude with an extensive analysis of the user studies and condense our findings in 10 key challenges that guide future work in this emerging research area.
                        </p>
                    </div>
                </div>
-->


                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/swivr-car-seat.png" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>SwiVR-Car-Seat: Exploring Vehicle Motion Effects on Interaction Quality in Virtual Reality Automated Driving Using a Motorized Swivel Seat</h2>
                        
                        <p> 
                            Mark Colley, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Enrico Rukzio, and Jan Gugenheimer
                        </p>
                        
                        <p><strong>IMWUT '21</strong>: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-6')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/swivr-car-seat.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="data/videos/swivr-car-seat.mp4" class="video">Video</a> /
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3494968" class="acm">ACM</a>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-6" style="display: none; padding-top: 5px; text-align: justify;">
                            Autonomous vehicles provide new input modalities to improve interaction with in-vehicle information systems. However, due to the road and driving conditions, the user input can be perturbed, resulting in reduced interaction quality. One challenge is assessing the vehicle motion effects on the interaction without an expensive high-fidelity simulator or a real vehicle. This work presents SwiVR-Car-Seat, a low-cost swivel seat to simulate vehicle motion using rotation. In an exploratory user study (N=18), participants sat in a virtual autonomous vehicle and performed interaction tasks using the input modalities touch, gesture, gaze, or speech. Results show that the simulation increased the perceived realism of vehicle motion in virtual reality and the feeling of presence. Task performance was not influenced uniformly across modalities; gesture and gaze were negatively affected while there was little impact on touch and speech. The findings can advise automotive user interface design to mitigate the adverse effects of vehicle motion on the interaction.
                        </p>
                    </div>
                </div>

<!--
                <div class="row">
                    <div class="col-md-6 text-center  " style="visibility: visible;  ">
                        <img src="images/stuck-continuum.png" />
                    </div>
                    
                    <div class="col-md-6  " style="visibility: visible;  ">
                        <h2>To Be or Not to Be Stuck, or Is It a Continuum?: A Systematic Literature Review on the Concept of Being Stuck in Games</h2>
                        
                        <p> 
                            Tobias Drey, Fabian Fischbach, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Julian Frommel, Michael Rietzler, and Enrico Rukzio
                        </p>
                        
                        <p><strong>CHI PLAY '21</strong>: Proceedings of the ACM on Human-Computer Interaction</p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                
                                <button onclick="toggleAbstract(this, 'abstract-5')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/stuck-continuum.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3474656" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        
                        <p id="abstract-5" style="display: none; padding-top: 5px;">
                            Players can get stuck in video games, which impedes their process to their goal and results in unfavorable outcomes like negative emotions, impediments of flow, and obstacles for learning. Currently, it is not easily possible to assess if a player is stuck, as no widely accepted definition of "being stuck" in games exists. We conducted 13 expert interviews and a systematic literature review with 104 relevant papers selected from 4022 candidates. We present a definition of "being stuck" that conceptualizes the state as a continuum and contextualize it within related concepts. Our stuck continuum can be applied to regulate the player's stuck level. We propose a taxonomy of measures that are useful for the detection of the level of stuckness and discuss the effectiveness of countermeasures. Our stuck concept is crucial for game developers creating an optimal player experience in games.    
                        </p>
                    </div>
                </div>
-->

                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/the-social-engineer.png" />
                    </div>
                
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>The Social Engineer: An Immersive Virtual Reality Educational Game to Raise Social Engineering Awareness</h2>
                
                        <p>
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u> and Fabian Fischbach
                        </p>
                
                        <!-- Conference Name with Custom Badge Behind -->
                        <div style="position: relative; display: inline-block;">
                            <p style="position: relative; z-index: 2; margin: 0;">
                                <strong>CHI PLAY '20</strong>: Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play, <b><div class="award">Audience Choice Award</div></b>
                            </p>
                        </div>
                
                        <div class="row" style="padding: 0; margin-top: 10px; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-4')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/the-social-engineer.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="data/videos/the-social-engineer.mp4" class="video">Video</a> /
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3383668.3419917" class="acm">ACM</a>
                        </div>
                
                        <!-- Hidden Abstract -->
                        <p id="abstract-4" style="display: none; padding-top: 5px; text-align: justify;">
                            As system infrastructures are becoming more secure against technical attacks, it is more difficult for attackers to overcome them with technical means. Social engineering instead exploits the human factor of information security and can have a significant impact on organizations. The lack of awareness about social engineering favors the successful realization of social engineering attacks, as employees do not recognize them as such early enough, resulting in high costs for the affected company. Current training approaches and awareness courses are limited in their versatility and create little motivation for employees to deal with the topic. The high immersion of virtual reality can improve learning in this context. We created The Social Engineer, an immersive educational game in virtual reality, to raise awareness and to sensitize players about social engineering. The player impersonates a penetration tester and conducts security audits in a virtually simulated company. The game consists of a detailed game world containing three distinct missions that require the player to apply different social engineering attack methods. Our concept enables the game to be highly extensible and flexible regarding different playable scenarios and settings. The Social Engineer can potentially benefit companies as an immersive self-training tool for their employees, support security experts in teaching social engineering awareness as part of a comprehensive training course, and entertain interested individuals by leveraging fun and innovative gameplay mechanics.
                        </p>
                    </div>
                </div>


                <div class="row">
                    <div class="col-md-4 text-center  " style="visibility: visible;  ">
                        <img src="images/shARe.png" />
                    </div>
                    
                    <div class="col-md-8  " style="visibility: visible;  ">
                        <h2>ShARe: Enabling Co-Located Asymmetric Multi-User Interaction for Augmented Reality Head-Mounted Displays</h2>
                        
                        <p> 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Fabian Fischbach, Jan Gugenheimer, Evgeny Stemasov, Julian Frommel, and Enrico Rukzio
                        </p>
                        
                        <p><strong>UIST '20</strong>: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</p>
                    
                        <div class="row" style="padding: 0; padding-left: 5px; margin-bottom: 10px;">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-3')" class="abstract-btn">Abstract</button> /
                                <a target="_blank" href="data/publications/shARe.pdf" class="paper">Paper</a> /
                                <a target="_blank" href="data/videos/shARe.mp4" class="video">Video</a> /
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3379337.3415843" class="acm">ACM</a>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-3" style="display: none; padding-top: 5px; text-align: justify;">
                            Head-Mounted Displays (HMDs) are the dominant form of enabling Virtual Reality (VR) and Augmented Reality (AR) for personal use. One of the biggest challenges of HMDs is the exclusion of people in the vicinity, such as friends or family. While recent research on asymmetric interaction for VR HMDs has contributed to solving this problem in the VR domain, AR HMDs come with similar but also different problems, such as conflicting information in visualization through the HMD and projection. In this work, we propose ShARe, a modified AR HMD combined with a projector that can display augmented content onto planar surfaces to include the outside users (non-HMD users). To combat the challenge of conflicting visualization between augmented and projected content, ShARe visually aligns the content presented through the AR HMD with the projected content using an internal calibration procedure and a servo motor. Using marker tracking, non-HMD users are able to interact with the projected content using touch and gestures. To further explore the arising design space, we implemented three types of applications (collaborative game, competitive game, and external visualization). ShARe is a proof-of-concept system that showcases how AR HMDs can facilitate interaction with outside users to combat exclusion and instead foster rich, enjoyable social interactions.    
                        </p>
                    </div>
                </div>


                <!--
                <div class="row">
                    <div class="col-md-6 text-center  " style="visibility: visible;  ">
                        <img src="images/adaptive-hints-educational.png" />
                    </div>
                    
                    <div class="col-md-6  " style="visibility: visible;  ">
                        <h2>Towards Progress Assessment for Adaptive Hints in Educational Virtual Reality Games</h2>
                        
                        <p>
                            Tobias Drey, 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Fabian Fischbach, Julian Frommel, and Enrico Rukzio
                        </p>
                        
                        <p>Proceedings of CHI EA 2020</p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <button onclick="toggleAbstract(this, 'abstract-2')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="/data/publications/adaptive-hints-educational.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/videos/adaptive-hints-educational.mp4" class="video">Video</a>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3334480.3382789" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        <p id="abstract-2" style="display: none; padding-top: 5px;">
                            One strength of educational games is their adaptivity to the individual learning progress. Methods to assess progress during gameplay are limited, especially in virtual reality (VR) settings, which show great potential for educational games because of their high immersion. We propose the concept of adaptive hints using progress assessment based on player behavior tracked through a VR-system's tracking capabilities. We implemented Social Engineer, a VR-based educational game teaching basic knowledge about social engineering (SE). In two user studies, we will evaluate the performance of the progress assessment and the effects of the intervention through adaptive hints on the players' experience and learning effects. This research can potentially benefit researchers and practitioners, who want to assess progress in educational games and leverage the real-time assessment for adaptive hint systems with the potential of improved player experience and learning outcomes.
                        </p>
                    </div>
                </div>-->

                <!--
                <div class="row">
                    <div class="col-md-6 text-center  " style="visibility: visible;  ">
                        <img src="images/proactive-dialogue.png" />
                    </div>
                    
                    <div class="col-md-6  " style="visibility: visible;  ">
                        <h2>A Comparison of Explicit and Implicit Proactive Dialogue Strategies for Conversational Recommendation</h2>
                        
                        <p>
                            Matthias Kraus, Fabian Fischbach, 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, 
                            and Wolfgang Minker
                        </p>
                        
                        <p>Proceedings of the Twelfth Language Resources and Evaluation Conference (LREC) 2020</p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <button onclick="toggleAbstract(this, 'abstract-1')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://aclanthology.org/2020.lrec-1.54/" class="paper">Paper</a> 
                            </div>
                        </div>
                        
                        <p id="abstract-1" style="display: none; padding-top: 5px;">
                            Recommendation systems aim at facilitating information retrieval for users by taking into account their preferences. Based on previous user behaviour, such a system suggests items or provides information that a user might like or find useful. Nonetheless, how to provide suggestions is still an open question. Depending on the way a recommendation is communicated influences the user’s perception of the system. This paper presents an empirical study on the effects of proactive dialogue strategies on user acceptance. Therefore, an explicit strategy based on user preferences provided directly by the user, and an implicit proactive strategy, using autonomously gathered information, are compared. The results show that proactive dialogue systems significantly affect the perception of human-computer interaction. Although no significant differences are found between implicit and explicit strategies, proactivity significantly influences the user experience compared to reactive system behaviour. The study contributes new insights to the human-agent interaction and the voice user interface design. Furthermore, interesting tendencies are discovered that motivate future work.
                        </p>
                    </div>
                </div>-->


                <hr>
                <div class="row">
                    <div class="col-md-12  " style="visibility: visible;  ">
                        <h1>Teaching</h1>
                        <div class="row">
                            <div class="col-md-6">                  
                                <strong>Research Project in Human-Computer Interaction</strong>
                                <p>Co-organized year-long, interdisciplinary team projects on user-centered design, culminating in multiple peer-reviewed publications.</p>
                                <p><em>Fall 2021 - Spring 2025</em></p>

                                <strong>User Interface Software Technologies</strong>
                                <p>Developed course materials and delivered weekly hands-on lectures covering interactive systems, formal HCI methods, and notation.</p>
                                <p><em>Spring 2022 - Spring 2024</em></p>

                                <strong>Automotive User Interfaces and Interactive Vehicle Applications</strong>
                                <p>Led weekly practical sessions and one lecture on future mobility, teaching design and evaluation of in-vehicle interfaces.</p>
                                <p><em>Fall 2021 - Fall 2024</em></p>

                                <strong>Research Trends in Media Informatics</strong>
                                <p>Co-organized the course, mentored PhD students on PRISMA literature surveys, and assessed research proposals.</p>
                                <p><em>Fall 2021 - Fall 2024</em></p>
                            </div>

                            <div class="col-md-6">
                                <img src="images/study-supervision.png" alt="Student supervision" />
                                <img src="images/team-supervision.png" alt="Team supervision" />
                            </div>
                        </div>

                        <h4>Guest Lecturing</h4>
                        <p></p>
                        2025-02-13 <b> UCL Interaction Centre, London, UK</b> invited by <i> Mark Colley </i> <br/>


                        <hr>
                        
                        <h1>Volunteering</h1>
                        Member of program committees:
                        <p></p>
                        <ul>
                            <li>CHI LBW '23 - '25</li>
                            <li>AutomotiveUI '24, '25</li>
                            <li>MuC '24, '25</li>
                            <li>CHIWORK '25</li>
                        </ul>
                        <p></p>
                        Member of organizing committees:
                        <p></p>
                        <ul>
                            <li>Registration and Local Chair AutomotiveUI '23</li>
                        </ul>
                        <p></p>
                        Reviewer (Excerpt):
                        <p></p>
                        <ul>
                            <li>Conference:
                                <ul>
                                    <li>AutomotiveUI '21 - '24</li>
                                    <li>CHI '22 - '25</li>
                                    <li>DIS '22, '23</li>
                                    <li>IEEE VIS '23</li>
                                    <li>CSCW '24</li>
                                    <li>UIST '24</li>
                                    <li>TEI '24, '25</li>
                                </ul>
                            </li>
                            
                            <li>Journal:
                                <ul>
                                    <li>IMWUT '22 - '25</li>
                                    <li>TVCG '22</li>
                                    <li>TRF '24</li>
                                    <li>BIT '24</li>
                                </ul>
                            </li>
                        </ul>
                        <p></p>
                        Special recognitions for reviews:
                        <p></p>
                        <ul>
                            <li>IMWUT 2022</li>
                            <li>ISS 2022</li>
                            <li>AutomotiveUI 2022</li>
                            <li>DIS 2023</li>
                            <li>IEEE VIS 2023</li>
                            <li>AutomotiveUI 2023</li>
                            <li>CHI 2024, <b>2x</b></li>
                            <li>CHI 2024 LBW</li>
                            <li>CSCW 2024</li>
                            <li>MuC 2024</li>
                            <li>UIST 2024</li>
                            <li>IMWUT 2024</li>
                            <li>CHI 2025</li>
                        </ul>
                        <p></p>
                        <p></p>

                        <hr>
                        <h1>Entrepreneurship</h1>
                        <div class="row">
                            <div class="col-md-9">                  
                                <h4>Co-Founder of <a href="https://zefwih.com/" target="_blank">Zefwih</a></h4>
                                <p>Supporting digitalization efforts and consult on future multimedia.</p>
                                <p>Created a serious game to guard against social engineering attacks: <a href="https://store.steampowered.com/app/2746510/The_Social_Engineer/" target="_blank">The Social Engineer</a> (available on Steam)</p>
                            </div>

                            <div class="col-md-3">
                                <img src="images/zefwih-banner.png" alt="Student Banner" />
                            </div>
                        </div>

                        <hr>
                        <br />
                    </div>
                </div>
            </div>
        </section>
        <!--  End Publication Section  -->
    </body>
</html>
