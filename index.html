<html lang="en-us" data-lt-installed="true">
    <head>
        <title>Pascal Jansen</title>
        <meta property="og:title" content="Pascal Jansen's portfolio" />
        <meta property="og:type" content="website" />
        <meta property="og:image" content="http://www.thijsroumen.eu/test/images/photos/background.jpg" />
        <meta property="og:description" content="Assistant Professor at Cornell Tech, director of the Matter of Tech lab" />
        <meta property="og:url" content="http://www.thijsroumen.com" />
        <meta name="description" content="Thijs Roumen is a researcher in Human Computer Interaction with a focus on reusing models for digital fabrication (3D printing, lasercutting)" />
        <meta
            name="keywords"
            content="Portable Fabrication, Laser cutting, Hasso Plattner Institute, HPI, HCI, Fabrication, 3D Printing, Reuse, Reuse for Digital Fabrication, Personal Fabrication, UIST, CHI, Design, Industrial Design, Interaction, Thijs Roumen, Roumen, Thijs, Eindhoven University of Technology, TU/e, University, Tsukuba, Tsukuba University, Seunghee Lee, Kansei, Technology, Product Design, Industrieel Ontwerp, Portfolio, SDU, University of Southern Denmark, IT Product Design, Stik Design, Kuchi, Yubi, Wiggle, Sonderborg, Syddansk Universitet, Denmark, HCI, NUS, National University of Singapore, NUS-HCI lab, Shengdong Zhao, Singapore, Research, PhD, Research Assistant, LEGO, DKNL*\design, Kompan, CHI 2015, Social Design Award, Sonderborg Entrepreneurship Award, Ivaerksaetter Award Sonderborg, iFabrikken"
        />
        <meta charset="utf-8" />
        <meta name="author" content="thijsroumen.eu" />
        <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0" />

        <link rel="stylesheet" type="text/css" href="css/bootstrap.css" />
        <link rel="stylesheet" type="text/css" href="css/animate.css" />
        <link rel="stylesheet" type="text/css" href="css/main.css" />

        <script type="text/javascript" src="js/jquery.js"></script>
        <script type="text/javascript" src="js/scrollTo.js"></script>
        <script type="text/javascript" src="js/wow.js"></script>
        <script type="text/javascript" src="js/parallax.js"></script>
        <script type="text/javascript" src="js/headhesive.js"></script>
        <script type="text/javascript" src="js/main.js"></script>
    </head>

    <body style="" data-new-gr-c-s-check-loaded="14.1215.0" data-gr-ext-installed="">
        <header class="banner">
            <div class="container">
                <div class="logo pull-left">
                    Pascal Jansen
                </div>

                <nav class="pull-right">
                    <ul class="list-unstyled">
                        <li><a target="_blank" href="data/2024-01-11_Pascal_Jansen_CV.pdf">CV</a></li>
                        <li><a target="_blank" href="mailto:pascal.jansen@uni-ulm.de">E-Mail</a></li>
                    </ul>
                </nav>
            </div>
        </header>

        <!--  Bio Section  -->
        <section class="bio" id="bio" style="background-color: rgb(224 224 224); text-align: left;">
            <div class="container">
                <div class="row equal-height">
                    <!-- Left column with text -->
                    <div class="col-md-8 panel">
                        <div class="caption">
                            <br />
                            <h1 class="animated wow fadeInUp" style="margin-top: 0;">Pascal Jansen</h1>
                        </div>

                        <p class="animated wow fadeInUp">
                            <span>pascal.jansen (at) uni-ulm.de</span>
                        </p>
                        <br />
                        <p class="animated wow fadeInUp">
                            <span>PhD Candidate and Research Associate at <a target="_blank" href="http://www.tech.cornell.edu/">Ulm University</a><br>Institute of Media Informatics, Human-Computer Interaction Group</span><br><br>
                            <span>Visiting Researcher at <a target="_blank" href="https://www.ucl.ac.uk/uclic/">University College London, Interaction Centre</a></span>
                        </p>
                        <br />
                        <div class="agenda animated wow fadeInUp">
                            <div class="bio-text-title"><h2>Ubiquitous Personalization</h2></div>
                            <p>
                                My research combines <strong>Human-Computer Interaction (HCI)</strong>, 
                                <strong>Computational Modeling</strong>, and <strong>Inclusive Design</strong> to create a future
                                where technology—whether in <strong>mobility</strong> contexts or everyday 
                                <strong>environments</strong>—is personalized seamlessly to each individual's states, preferences, 
                                and needs. A key challenge lies in developing systems that effectively respond 
                                to diverse user abilities and hardly predictable use contexts. By advancing 
                                <strong>computational user interface optimization</strong>, 
                                <strong>scalable simulations and evaluations</strong>, and 
                                <strong>inclusive design</strong>, I aim to make 
                                technology more intuitive, adaptive, and accessible for all.
                              </p>
                            <br />
                            <!-- Left-aligned links -->
                            <div class="horizontal-list bio-link-list-wide">
                                <a target="_blank" href="data/2024-01-11_Pascal_Jansen_CV.pdf">Curriculum vitae (November 2024)</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://scholar.google.de/citations?user=cR1_0-EAAAAJ&hl=en">Scholar Profile</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://www.linkedin.com/in/pascal-jansen-/">LinkedIn</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://x.com/pascal_jansen_">X</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://github.com/Pascal-Jansen">GitHub</a>
                            </div>
                            <div class="horizontal-list bio-link-list-narrow">
                                <a target="_blank" href="data/2024-01-11_Pascal_Jansen_CV.pdf">CV</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://scholar.google.de/citations?user=cR1_0-EAAAAJ&hl=en">Scholar</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://www.linkedin.com/in/pascal-jansen-/">LinkedIn</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://x.com/pascal_jansen_">X</a>
                                <span class="separator">/</span>
                                <a target="_blank" href="https://github.com/Pascal-Jansen">GitHub</a>
                            </div>
                        </div>
                    </div>

                    <div class="col-md-4 align-bottom">
                        <img src="images/uu-jansen-transparent.png" alt="Profile Picture" class="profile-image" />
                    </div>
                </div>
            </div>
        </section>
        <!--  End Bio Section  -->

        <a target="_blank" id="showHere"></a>

        <!--  Publication Section  -->
        <section class="publication" id="publication">
            <div class="container">
     <!--
                <h1>News</h1>
                <div class="row">
                  <div class="col-md-3">
                    <h4 style="margin-top: 0;">November 2024</h4>
                  </div>
                
                  <div class="col-md-9">
                    News content goes here...
                  </div>
                </div>


                <hr>
                <h1>Research Agenda</h1>
                <div class="row">
                    <div class="col-md-12">
                        This is an empty space to paste the research agenda ...

                    </div>
                </div>
            -->

                <hr>
                <h1>Publications (Excerpt)</h1>
                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/edulicit.jpg" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>Visualizing Imperfect Situation Detection and Prediction in Automated Vehicles: Understanding Users’ Perceptions via User-Chosen Scenarios</h2>
                        
                        <p> 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen*</a></u>, Mark Colley*, Tim Pfeifer, and Enrico Rukzio (<i>* joint first-author</i>)
                        </p>
                        
                        <p><i>Transportation Research Part F: Psychology and Behavior (TRF) 2024</i></p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-13')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/edulicit.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S1369847824001141">Elsevier</a>
                            </div>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-13" style="display: none; padding-top: 5px;">
                            User acceptance is essential for successfully introducing automated vehicles (AVs). Understanding the technology is necessary to overcome skepticism and achieve acceptance. This could be achieved by visualizing (uncertainties of) AV's internal processes, including situation perception, prediction, and trajectory planning. At the same time, relevant scenarios for communicating the functionalities are unclear. Therefore, we developed EduLicitto concurrently elicit relevant scenarios and evaluate the effects of visualizing AV's internal processes. A website capable of showing annotated videos enabled this methodology. With it, we replicated the results of a previous online study (N=76) using pre-recorded real-world videos. Additionally, in a second online study (N=22), participants uploaded scenarios they deemed challenging for AVs using our website. Most scenarios included large intersections and/or multiple vulnerable road users. Our work helps assess scenarios perceived as challenging for AVs by the public and, simultaneously, can help educate the public about visualizations of the functionalities of current AVs.
                        </p>
                    </div>
                </div>



                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/ped-sumo.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>PedSUMO: Simulacra of Automated Vehicle-Pedestrian Interaction Using SUMO To Study Large-Scale Effects</h2>
                        
                        <p> 
                            Mark Colley, Julian Czymmeck, Mustafa Kücükkocak, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, and Enrico Rukzio
                        </p>
                        
                        <p><i>Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2024</i></p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-12')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/ped-sumo.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3610977.3637478" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-12" style="display: none; padding-top: 5px;">
                            As automated vehicles become more widespread but lack a driver to communicate in uncertain situations, external communication, for example, via LEDs or displays, is evaluated. However, the concepts are mostly evaluated in simple scenarios, such as one person trying to cross in front of one automated vehicle. The traditional empirical approach fails to study the large-scale effects of these in this not-yet-real scenario. Therefore, we built PedSUMO, an enhancement to SUMO for the simulacra of automated vehicles' effects on public traffic, specifically how pedestrian attributes affect their respect for automated vehicle priority at unprioritized crossings. We explain the algorithms used and the derived parameters relevant to the crossing. We open-source our code under https://github.com/M-Colley/pedsumo and demonstrate an initial data collection and analysis of Ingolstadt, Germany.
                        </p>
                    </div>
                </div>



                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/esfs.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>'Eco Is Just Marketing': Unraveling Everyday Barriers to the Adoption of Energy-Saving Features in Major Home Appliances</h2>
                        
                        <p> 
                            Albin Zeqiri, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Jan Ole Rixen, Michael Rietzler, and Enrico Rukzio
                        </p>
                        
                        <p><i>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) 2024</i></p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-11')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/esfs.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3643558" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-11" style="display: none; padding-top: 5px;">
                            Energy-saving features (ESFs) represent a simple way to reduce the resource consumption of home appliances (HAs), yet they remain under-utilized. While prior research focused on increasing the use of ESFs through behavior change interventions, there is currently no clarity on the barriers that restrict their utilization in the first place. To bridge this gap, we conducted a qualitative analysis of 349 Amazon product reviews and 98 Reddit discussions, yielding three qualitative themes that showcase how users perceive, interact with, and evaluate ESFs in HAs. Based on these themes, we derived frequent barriers to ESF adoption, which guided a subsequent expert focus group (N=5) to assess the suitability of behavior change interventions and potential alternative strategies for ESF adoption. Our findings deepen the understanding of everyday barriers surrounding ESFs and enable the targeted design and assessment of interventions for future HAs.
                        </p>
                    </div>
                </div>


                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/eye-gaze-game.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>Effects of a Gaze-Based 2D Platform Game on User Enjoyment, Perceived Competence, and Digital Eye Strain</h2>
                        
                        <p> 
                            Mark Colley, Beate Wanner, Max Rädler, Marcel Rötzer, Julian Frommel, Teresa Hirzle, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, and Enrico Rukzio
                        </p>
                        
                        <p><i>CHI '24: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems</i></p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-10')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/eye-gaze-game.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/videos/eye-gaze-game.mp4" class="video">Video</a>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/full/10.1145/3613904.3641909" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-10" style="display: none; padding-top: 5px;">
                            Gaze interaction is a promising interaction method to increase variety, challenge, and fun in games. We present “Shed Some Fear”, a 2D platform game including numerous eye-gaze-based interactions. “Shed Some Fear” includes control with eye-gaze and traditional keyboard input. The eye-gaze interactions are partially based on eye exercises reducing digital eye strain but also on employing peripheral vision. By employing eye-gaze as a necessary input mechanism, we explore the effects on and tradeoffs between user enjoyment and digital eye strain in a five-day longitudinal between-subject study (N=17) compared to interaction with a traditional mouse. We found that perceived competence was significantly higher with eye gaze interaction and significantly higher internal eye strain. With this work, we contribute to the not straightforward inclusion of eye tracking as a useful and fun input method for games.
                        </p>
                    </div>
                </div>



                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/AutoVis.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>AutoVis: Enabling Mixed-Immersive Analysis of Automotive User Interface Interaction Studies</h2>
                        
                        <p> 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Julian Britten, Alexander Häusele, Thilo Segschneider, Mark Colley, and Enrico Rukzio
                        </p>
                        
                        <p><i>CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</i></p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-9')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/AutoVis.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/videos/AutoVis.mp4" class="video">Video</a>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/full/10.1145/3544548.3580760" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-9" style="display: none; padding-top: 5px;">
                            Automotive user interface (AUI) evaluation becomes increasingly complex due to novel interaction modalities, driving automation, heterogeneous data, and dynamic environmental contexts. Immersive analytics may enable efficient explorations of the resulting multilayered interplay between humans, vehicles, and the environment. However, no such tool exists for the automotive domain. With AutoVis, we address this gap by combining a non-immersive desktop with a virtual reality view enabling mixed-immersive analysis of AUIs. We identify design requirements based on an analysis of AUI research and domain expert interviews (N=5). AutoVis supports analyzing passenger behavior, physiology, spatial interaction, and events in a replicated study environment using avatars, trajectories, and heatmaps. We apply context portals and driving-path events as automotive-specific visualizations. To validate AutoVis against real-world analysis tasks, we implemented a prototype, conducted heuristic walkthroughs using authentic data from a case study and public datasets, and leveraged a real vehicle in the analysis process.
                        </p>
                    </div>
                </div>



                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/auto-design-space.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review</h2>
                        
                        <p> 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Mark Colley, and Enrico Rukzio
                        </p>
                        
                        <p><i>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) 2022</i></p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-8')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/auto-design-space.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3534617" class="acm">ACM</a>
                            </div>
                            <div class="col-sm-3" style="padding-top: 5px;">
                                <a target="_blank" href="https://in-vehicle-interaction-design-space.onrender.com/" class="paper">Website</a>
                            </div>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-8" style="display: none; padding-top: 5px;">
                            Automotive user interfaces constantly change due to increasing automation, novel features, additional applications, and user demands. While in-vehicle interaction can utilize numerous promising modalities, no existing overview includes an extensive set of human sensors and actuators and interaction locations throughout the vehicle interior. We conducted a systematic literature review of 327 publications leading to a design space for in-vehicle interaction that outlines existing and lack of work regarding input and output modalities, locations, and multimodal interaction. To investigate user acceptance of possible modalities and locations inferred from existing work and gaps unveiled in our design space, we conducted an online study (N=48). The study revealed users' general acceptance of novel modalities (e.g., brain or thermal activity) and interaction with locations other than the front (e.g., seat or table). Our work helps practitioners evaluate key design decisions, exploit trends, and explore new areas in the domain of in-vehicle interaction.
                        </p>
                    </div>
                </div>



                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/eye-strain.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>Understanding, Addressing, and Analysing Digital Eye Strain in Virtual Reality Head-Mounted Displays</h2>
                        
                        <p> 
                            Teresa Hirzle, Fabian Fischbach, Julian Karlbauer, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Jan Gugenheimer, Enrico Rukzio, and Andreas Bulling
                        </p>
                        
                        <p><i>ACM Transactions on Computer-Human Interaction (TOCHI) 2022</i></p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-7')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/eye-strain.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/full/10.1145/3492802" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-7" style="display: none; padding-top: 5px;">
                            Digital eye strain (DES), caused by prolonged exposure to digital screens, stresses the visual system and negatively affects users’ well-being and productivity. While DES is well-studied in computer displays, its impact on users of virtual reality (VR) head-mounted displays (HMDs) is largely unexplored—despite that some of their key properties (e.g., the vergence-accommodation conflict) make VR-HMDs particularly prone. This work provides the first comprehensive investigation into DES in VR HMDs. We present results from a survey with 68 experienced users to understand DES symptoms in VR-HMDs. To help address DES, we investigate eye exercises resulting from survey answers and blue light filtering in three user studies (N = 71). Results demonstrate that eye exercises, but not blue light filtering, can effectively reduce DES. We conclude with an extensive analysis of the user studies and condense our findings in 10 key challenges that guide future work in this emerging research area.
                        </p>
                    </div>
                </div>



                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/swivr-car-seat.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>SwiVR-Car-Seat: Exploring Vehicle Motion Effects on Interaction Quality in Virtual Reality Automated Driving Using a Motorized Swivel Seat</h2>
                        
                        <p> 
                            Mark Colley, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Enrico Rukzio, and Jan Gugenheimer
                        </p>
                        
                        <p><i>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) 2021</i></p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-6')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/swivr-car-seat.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/videos/swivr-car-seat.mp4" class="video">Video</a>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3494968" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-6" style="display: none; padding-top: 5px;">
                            Autonomous vehicles provide new input modalities to improve interaction with in-vehicle information systems. However, due to the road and driving conditions, the user input can be perturbed, resulting in reduced interaction quality. One challenge is assessing the vehicle motion effects on the interaction without an expensive high-fidelity simulator or a real vehicle. This work presents SwiVR-Car-Seat, a low-cost swivel seat to simulate vehicle motion using rotation. In an exploratory user study (N=18), participants sat in a virtual autonomous vehicle and performed interaction tasks using the input modalities touch, gesture, gaze, or speech. Results show that the simulation increased the perceived realism of vehicle motion in virtual reality and the feeling of presence. Task performance was not influenced uniformly across modalities; gesture and gaze were negatively affected while there was little impact on touch and speech. The findings can advise automotive user interface design to mitigate the adverse effects of vehicle motion on the interaction.
                        </p>
                    </div>
                </div>


                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/stuck-continuum.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>To Be or Not to Be Stuck, or Is It a Continuum?: A Systematic Literature Review on the Concept of Being Stuck in Games</h2>
                        
                        <p> 
                            Tobias Drey, Fabian Fischbach, <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Julian Frommel, Michael Rietzler, and Enrico Rukzio
                        </p>
                        
                        <p><i>Proceedings of the ACM on Human-Computer Interaction (CHI PLAY) 2021</i></p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-5')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/stuck-continuum.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3474656" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-5" style="display: none; padding-top: 5px;">
                            Players can get stuck in video games, which impedes their process to their goal and results in unfavorable outcomes like negative emotions, impediments of flow, and obstacles for learning. Currently, it is not easily possible to assess if a player is stuck, as no widely accepted definition of "being stuck" in games exists. We conducted 13 expert interviews and a systematic literature review with 104 relevant papers selected from 4022 candidates. We present a definition of "being stuck" that conceptualizes the state as a continuum and contextualize it within related concepts. Our stuck continuum can be applied to regulate the player's stuck level. We propose a taxonomy of measures that are useful for the detection of the level of stuckness and discuss the effectiveness of countermeasures. Our stuck concept is crucial for game developers creating an optimal player experience in games.    
                        </p>
                    </div>
                </div>


                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/the-social-engineer.png" />
                    </div>
                
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>The Social Engineer: An Immersive Virtual Reality Educational Game to Raise Social Engineering Awareness</h2>
                
                        <p>
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u> and Fabian Fischbach
                        </p>
                
                        <!-- Conference Name with Custom Badge Behind -->
                        <div style="position: relative; display: inline-block;">
                            <p style="position: relative; z-index: 2; margin: 0;">
                                <i>CHI PLAY '20: Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play</i>, <div class="award">Audience Choice Award</div>
                            </p>
                        </div>
                
                        <div class="row" style="padding: 0; margin-top: 10px;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-4')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/the-social-engineer.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/videos/the-social-engineer.mp4" class="video">Video</a>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3383668.3419917" class="acm">ACM</a>
                            </div>
                        </div>
                
                        <!-- Hidden Abstract -->
                        <p id="abstract-4" style="display: none; padding-top: 5px;">
                            As system infrastructures are becoming more secure against technical attacks, it is more difficult for attackers to overcome them with technical means. Social engineering instead exploits the human factor of information security and can have a significant impact on organizations. The lack of awareness about social engineering favors the successful realization of social engineering attacks, as employees do not recognize them as such early enough, resulting in high costs for the affected company. Current training approaches and awareness courses are limited in their versatility and create little motivation for employees to deal with the topic. The high immersion of virtual reality can improve learning in this context. We created The Social Engineer, an immersive educational game in virtual reality, to raise awareness and to sensitize players about social engineering. The player impersonates a penetration tester and conducts security audits in a virtually simulated company. The game consists of a detailed game world containing three distinct missions that require the player to apply different social engineering attack methods. Our concept enables the game to be highly extensible and flexible regarding different playable scenarios and settings. The Social Engineer can potentially benefit companies as an immersive self-training tool for their employees, support security experts in teaching social engineering awareness as part of a comprehensive training course, and entertain interested individuals by leveraging fun and innovative gameplay mechanics.
                        </p>
                    </div>
                </div>


                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/shARe.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>ShARe: Enabling Co-Located Asymmetric Multi-User Interaction for Augmented Reality Head-Mounted Displays</h2>
                        
                        <p> 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Fabian Fischbach, Jan Gugenheimer, Evgeny Stemasov, Julian Frommel, and Enrico Rukzio
                        </p>
                        
                        <p><i>UIST '20: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</i></p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <!-- Abstract Button -->
                                <button onclick="toggleAbstract(this, 'abstract-3')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/publications/shARe.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/videos/shARe.mp4" class="video">Video</a>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3379337.3415843" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        <!-- Hidden Abstract -->
                        <p id="abstract-3" style="display: none; padding-top: 5px;">
                            Head-Mounted Displays (HMDs) are the dominant form of enabling Virtual Reality (VR) and Augmented Reality (AR) for personal use. One of the biggest challenges of HMDs is the exclusion of people in the vicinity, such as friends or family. While recent research on asymmetric interaction for VR HMDs has contributed to solving this problem in the VR domain, AR HMDs come with similar but also different problems, such as conflicting information in visualization through the HMD and projection. In this work, we propose ShARe, a modified AR HMD combined with a projector that can display augmented content onto planar surfaces to include the outside users (non-HMD users). To combat the challenge of conflicting visualization between augmented and projected content, ShARe visually aligns the content presented through the AR HMD with the projected content using an internal calibration procedure and a servo motor. Using marker tracking, non-HMD users are able to interact with the projected content using touch and gestures. To further explore the arising design space, we implemented three types of applications (collaborative game, competitive game, and external visualization). ShARe is a proof-of-concept system that showcases how AR HMDs can facilitate interaction with outside users to combat exclusion and instead foster rich, enjoyable social interactions.    
                        </p>
                    </div>
                </div>


                <!--
                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/adaptive-hints-educational.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>Towards Progress Assessment for Adaptive Hints in Educational Virtual Reality Games</h2>
                        
                        <p>
                            Tobias Drey, 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, Fabian Fischbach, Julian Frommel, and Enrico Rukzio
                        </p>
                        
                        <p><i>Proceedings of CHI EA</i> 2020</p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <button onclick="toggleAbstract(this, 'abstract-2')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="/data/publications/adaptive-hints-educational.pdf" class="paper">Paper</a> 
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="data/videos/adaptive-hints-educational.mp4" class="video">Video</a>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3334480.3382789" class="acm">ACM</a>
                            </div>
                        </div>
                        
                        <p id="abstract-2" style="display: none; padding-top: 5px;">
                            One strength of educational games is their adaptivity to the individual learning progress. Methods to assess progress during gameplay are limited, especially in virtual reality (VR) settings, which show great potential for educational games because of their high immersion. We propose the concept of adaptive hints using progress assessment based on player behavior tracked through a VR-system's tracking capabilities. We implemented Social Engineer, a VR-based educational game teaching basic knowledge about social engineering (SE). In two user studies, we will evaluate the performance of the progress assessment and the effects of the intervention through adaptive hints on the players' experience and learning effects. This research can potentially benefit researchers and practitioners, who want to assess progress in educational games and leverage the real-time assessment for adaptive hint systems with the potential of improved player experience and learning outcomes.
                        </p>
                    </div>
                </div>-->

                <!--
                <div class="row">
                    <div class="col-md-6 text-center animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <img src="images/proactive-dialogue.png" />
                    </div>
                    
                    <div class="col-md-6 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h2>A Comparison of Explicit and Implicit Proactive Dialogue Strategies for Conversational Recommendation</h2>
                        
                        <p>
                            Matthias Kraus, Fabian Fischbach, 
                            <u><a target="_blank" href="https://www.pascal-jansen.github.io" style="text-decoration: none; margin-right: 0px;">Pascal Jansen</a></u>, 
                            and Wolfgang Minker
                        </p>
                        
                        <p><i>Proceedings of the Twelfth Language Resources and Evaluation Conference (LREC)</i> 2020</p>
                    
                        <div class="row" style="padding: 0;">
                            <div class="col-sm-3">
                                <button onclick="toggleAbstract(this, 'abstract-1')" class="abstract-btn">Abstract</button>
                            </div>
                            <div class="col-sm-2" style="padding-top: 5px;">
                                <a target="_blank" href="https://aclanthology.org/2020.lrec-1.54/" class="paper">Paper</a> 
                            </div>
                        </div>
                        
                        <p id="abstract-1" style="display: none; padding-top: 5px;">
                            Recommendation systems aim at facilitating information retrieval for users by taking into account their preferences. Based on previous user behaviour, such a system suggests items or provides information that a user might like or find useful. Nonetheless, how to provide suggestions is still an open question. Depending on the way a recommendation is communicated influences the user’s perception of the system. This paper presents an empirical study on the effects of proactive dialogue strategies on user acceptance. Therefore, an explicit strategy based on user preferences provided directly by the user, and an implicit proactive strategy, using autonomously gathered information, are compared. The results show that proactive dialogue systems significantly affect the perception of human-computer interaction. Although no significant differences are found between implicit and explicit strategies, proactivity significantly influences the user experience compared to reactive system behaviour. The study contributes new insights to the human-agent interaction and the voice user interface design. Furthermore, interesting tendencies are discovered that motivate future work.
                        </p>
                    </div>
                </div>-->


                <hr>
                <div class="row">
                    <div class="col-md-8 animated wow fadeInUp animated" style="visibility: visible; animation-name: fadeInUp;">
                        <h1>Guest Lecturing</h1>
                        <p>
                            2025-02-13 <b> UCL Interaction Centre, London, UK</b> invited by <i> Mark Colley </i> <br/>
                        </p>


                        <hr>
                        <h1>Volunteering</h1>
                        Member of program committees:
                        <p></p>
                        <ul>
                            <li>CHI LBW '23 - '25</li>
                            <li>AutomotiveUI '24</li>
                            <li>MuC '24</li>
                            <li>CHIWORK '25</li>
                        </ul>
                        <p></p>
                        Member of organizing committees:
                        <p></p>
                        <ul>
                            <li>Registration and Local Chair AutomotiveUI '23</li>
                        </ul>
                        <p></p>
                        Reviewer (Excerpt):
                        <p></p>
                        <ul>
                            <li>Conference:
                                <ul>
                                    <li>AutomotiveUI '21 - '24</li>
                                    <li>CHI '22 - '25</li>
                                    <li>DIS '22, '23</li>
                                    <li>IEEE VIS '23</li>
                                    <li>CSCW '24</li>
                                    <li>UIST '24</li>
                                    <li>TEI '24, '25</li>
                                </ul>
                            </li>
                            
                            <li>Journal:
                                <ul>
                                    <li>IMWUT '22 - '25</li>
                                    <li>TVCG '22</li>
                                    <li>TRF '24</li>
                                    <li>BIT '24</li>
                                </ul>
                            </li>
                        </ul>
                        <p></p>
                        Special recognitions for reviews:
                        <p></p>
                        <ul>
                            <li>IMWUT 2022</li>
                            <li>ISS 2022</li>
                            <li>AutomotiveUI 2022</li>
                            <li>DIS 2023</li>
                            <li>IEEE VIS 2023</li>
                            <li>AutomotiveUI 2023</li>
                            <li>CHI 2024, <b>2x</b></li>
                            <li>CHI 2024 LBW</li>
                            <li>CSCW 2024</li>
                            <li>MuC 2024</li>
                            <li>UIST 2024</li>
                            <li>CHI 2025</li>
                            <li>IMWUT 2024</li>
                        </ul>
                        <p></p>
                        <p></p>
<!--
                        <hr>
                        <h1>Other</h1>
                        <p>
                            
                        </p>

                        <hr>
                        <br />-->
                    </div>
                </div>
            </div>
        </section>
        <!--  End Publication Section  -->
    </body>
</html>